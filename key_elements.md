**Learning Objectives**: 

*LO3a: Learn about the nature of reproducible research, workflow design, data management and manipulation, dynamic reporting, what the key requirements are, and which resources are available to support these (knowledge).*

*LO3b: Be able to use available resources to create a workflow for reproducible research (task).*

### Key components:

* Factors that affect reproducibility of research.

* Principles of reproducibility, and integrity and ethics in research.

* What is the 'reproducibility crisis', and meta-analyses of reproducibility.

* Open materials, reagents and hardware, including resources, repositories and standards.

* Electronic lab notebooks.

* Data analysis documentation and open research workflows.

* Living figures, turning scripts into reproducible documents, and Markdown.

* Pre-registration and prevention of p-hacking/HARK-ing (Hypothesising After Results are Known).

* Reproducible analysis environments (virtualization).

* What are the computing options and environments that allow collaborative and reproducible set up.

### Who to involve:

* Individuals: Andy Byers, Anna Krystalli, Julien Colomb, Rutger Vos, Brian Nosek, Lorena Barba, Karl Broman, Victoria Stodden, John Ioannidis, Chris Chambers.

* Organisations: [FOSTER](https://www.fosteropenscience.eu/), Center for Open Science, [COPE](https://publicationethics.org/), [Protocols.io](https://www.protocols.io/), ROpenSci, [Addgene](addgene.org), [BITSS](https://www.bitss.org/), [Project TIER](https://www.projecttier.org/).

* Other: [GOSH](http://openhardware.science/) Community, Software and Data Carpentry communities.

### Key resources:

**Tools**

* Open Science Framework (COS).

    * [Reproducibility Project: Cancer Biology](https://osf.io/e81xl/wiki/home/).

    * [Reproducibility Project: Psychological Science](https://osf.io/ezcuj/wiki/home/).

    * [Registered Reports](https://cos.io/rr/).

* Existing reproducible research workshops/practical resources:

    * [Reproducible Research Workshop](https://docs.google.com/presentation/d/1QfJVeim797fLBBAE003W6cqtahkGdJqa2GSsmm-t89s/edit?usp=sharing) (CC-BY, April Clyburne-Sherin & Courtney Soderberg).

    * [Initial steps towards reproducible research](http://kbroman.org/steps2rr/) (Karl Broman).

    * [The Open Science and Reproducible Research course](https://github.com/cbahlai/OSRR_course) (CC-BY, Christie Bahlai).

    * [Reproducibility Workshop](https://codeocean.com/workshop/caltech): Best practices and easy steps to save time for yourself and other researchers ([Code Ocean](https://codeocean.com/)).

    * [Reproducibility in Science: A guide to enhancing reproducibility in scientific results and writing](http://ropensci.github.io/reproducibility-guide/), ROpenSci.

    * [Reproducible Research using Jupyter Notebooks workshop](https://reproducible-science-curriculum.github.io/workshop-RR-Jupyter/) (Data Carpentry).

    * [R markdown workshop](https://github.com/libscie/rmarkdown-workshop) (Liberate Science).

    * [rrtools: Tools for Writing Reproducible Research in R](https://github.com/benmarwick/rrtools) (Ben Marwick).

* [ReproZip](https://reprozip.org), an open source tool for full computational reproducibility.

* [Software Carpentry](https://software-carpentry.org/lessons/) and [Data Carpentry](http://www.datacarpentry.org/lessons/) lessons.

* [Jupyter notebooks](http://jupyter.org/) (and [JupyterLab](https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906)), [R Markdown](https://www.rstudio.com/resources/webinars/getting-started-with-r-markdown/), [Stencila](https://stenci.la/).

* Virtual Machines, [Docker](https://www.docker.com/), [Vagrant](https://www.vagrantup.com/), [Binder](https://github.com/jupyterhub/binderhub)[Hub](https://github.com/jupyterhub/binderhub), [nteract.io](https://nteract.io/).

    * [Binder Documentation](https://mybinder.readthedocs.io/en/latest/), for creating custom computing environments that can be shared and used by multiple remote users.

* [Statcheck](http://statcheck.io/), GRIM.

* [Scienceroot](https://www.scienceroot.com/), the first blockchain-based scientific ecosystem.

* Online repositories for open hardware: [PLOS open source toolkit channel](https://channels.plos.org/open-source-toolkit); [Open Neuroscience](http://www.openeuroscience.com); [Open Plant Science](http://openplant.science/); [Appropedia](http://www.appropedia.org/Welcome_to_Appropedia); [DocuBricks](http://www.docubricks.com/); [Hackaday.io](https://hackaday.io/submissions/prize2016_citizen/list).

* [Bio-protocol](https://bio-protocol.org/Default.aspx), a peer reviewed protocol journal.

* [BMJ Open Science](http://openscience.bmj.com/), a new journal that aims to improve the transparency, integrity and reproducibility of biomedical research.

* [Evernote](https://evernote.com/), [Labguru](https://www.labguru.com/), [sciNote](https://scinote.net/).

* [AsPredicted](https://aspredicted.org/).

* The [Sci-Gaia Open Science Platform](http://www.sci-gaia.eu/osp/).

* [Improving your statistical inferences](https://www.coursera.org/learn/statistical-inferences), Daniel Lakens.

    * [Open Stats Lab](https://sites.trinity.edu/osl/data-sets-and-activities), Kevin McIntyre.

* [R for Data Science](http://r4ds.had.co.nz/).

    * [R tutorial: Introduction to cleaning data with R](https://www.youtube.com/watch?v=6PVMJE3HBN0) (DataCamp).

* [Nextflow](https://www.nextflow.io/), open source tool than enables reproducible and portable computational workflows across cloud and clusters.

**Research Articles and Reports**

* [Reproducibility, Virtual Appliances, and Cloud Computing](https://osf.io/sp2vg/) (Howe, 2012).

* [The Ironic Effect of Significant Results on the Credibility of Multiple-Study Articles](http://datacolada.org/wp-content/uploads/2014/06/3644-Schimmack-PM-2012-the-ironic-effect-of-significant-results-on-the-credibilit-of-multiple-study-articles.pdf) (Schimmack, 2012).

* [Power failure: why small sample size undermines the reliability of neuroscience](https://www.nature.com/articles/nrn3475) (Button et al., 2013).

* [Git can facilitate greater reproducibility and increased transparency in science](https://scfbm.biomedcentral.com/articles/10.1186/1751-0473-8-7) (Ram, 2013).

* [Ten simple rules for reproducible computational research](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285) (Sandve et al., 2013).

* [Investigating Variation in Replicability: A "Many Labs" Replication Project](https://econtent.hogrefe.com/doi/abs/10.1027/1864-9335/a000178) (Klein et al., 2014).

* [An introduction to Docker for reproducible research](https://arxiv.org/pdf/1410.0846.pdf) (Boettiger, 2015).

* [Opinion: Reproducible research can still be wrong: Adopting a prevention approach](http://www.pnas.org/content/112/6/1645) (Leek and Peng, 2015).

* [Replicability vs. reproducibility - or is it the other way around?](http://languagelog.ldc.upenn.edu/nll/?p=21956) (Liberman, 2015).

* [The GRIM test: A simple technique detects numerous anomalies in the reporting of results in psychology](https://peerj.com/preprints/2064v1/) (Brown and Heathers, 2016).

* [What does research reproducibility mean?](http://stm.sciencemag.org/content/8/341/341ps12) (Goodman et al., 2016).

* [Tools and techniques for computational reproducibility](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4940747/) (Piccolo and Frampton, 2016).

* [Transparency, Reproducibility, and the Credibility of Economics Research](https://osf.io/preprints/bitss/9a3rw/) (Christensen and Miguel, 2017).

* [A trust approach for sharing research reagents](http://stm.sciencemag.org/content/9/392/eaai9055.full?ijkey=uMGKxsCEiOb5s&keytype=ref&siteid=scitransmed) (Edwards et al., 2017).

* [Estimating the Reproducibility of Psychological Science](https://osf.io/447b3/) (Nosek et al., 2017).

* [Digital Open Science: Teaching digital tools for reproducible and transparent research](https://osf.io/fbvep/) (Toelch and Ostwald, 2017).

* [Terminologies for reproducible research](https://arxiv.org/abs/1802.03311) (Barba, 2018).

* [An introduction to statistical and data sciences via R](https://ismayc.github.io/moderndiver-book/) (Ismay and Kim, 2018).

* [The practice of reproducible research: case studies and lessons from the data-intensive sciences](https://www.practicereproducibleresearch.org/) (Kitzes et al., 2018).

* [bookdown: Authoring Books and Technical Documents with R Markdown](https://bookdown.org/yihui/bookdown/) (Xie, 2018).

* [Our path to better science in less time using open data science tools](https://www.nature.com/articles/s41559-017-0160) (Lowndes et al. 2017).

* [Haves and Have nots must find a better way: The case for Open Scientific Hardware](https://osf.io/uh682/) (Chagas, 2018).

* [Computational Reproducibility via Containers in Social Psychology](https://psyarxiv.com/mf82t) (Green and Clyburne-Sherin, 2018).

* Recommendations for open data science (Melissa Gymrek and Yossi Farjoun, 2016)

**Key Posts**

* Data hygiene and data provenance.

    * [A Data Cleaner's Cookbook](https://www.polydesmida.info/cookbook/).

    * [Storify by Dawn Bazely](https://storify.com/dawnbazely/data-hygeine-dataprovenance-harvardforest).

* [Failure is moving science forward](https://fivethirtyeight.com/features/failure-is-moving-science-forward/), Christie Aschwanden.

* [5 keys to building open hardware](https://opensource.com/article/18/2/5-steps-creating-successful-open-hardware), Joshua Pearce.

* [How to make replication the norm](https://www.nature.com/articles/d41586-018-02108-9) (Gertler et al., 2018).

* [Reproducibility PI Manifesto](http://lorenabarba.com/gallery/reproducibility-pi-manifesto/), Lorena Barba.

    * [How to run a lab for reproducible research](https://figshare.com/articles/How_to_run_a_lab_for_reproducible_research/4676170/1), Lorena Barba.

    * [Essential skills for reproducible research computing](https://barbagroup.github.io/essential_skills_RRC/) (Barba et al., 2017).

**Other**

* Institutions, projects, and companies using or providing open hardware/materials: CERN's [Open Hardware Repository](http://www.ohwr.org/) and Open Hardware License; [UFRGS Centro de Tecnologia AcadÃªmica](http://cta.if.ufrgs.br) (CTA); [Michigan Tech Open Sustainability Technology research group](http://www.mse.mtu.edu/~pearce/Index.html); [Open Plant](https://www.openplant.org/); [Trend in Africa](www.trendinafrica.org); [Open Lab Tools Cambridge University](http://openlabtools.eng.cam.ac.uk/index.php); [PhotosynQ](https://photosynq.org/); [PublicLab](https://publiclab.org/); [BackyardBrains](https://backyardbrains.com/); [OpenPCR](http://www.openpcr.org); [OpenROV](https://www.openrov.com/); [Prometheus Science](http://www.prometheus-science.com); [senseBox](https://sensebox.de/en/), [Addgene](addgene.org)

* [Definition of Open Reproducible Research](https://www.fosteropenscience.eu/taxonomy/term/103), FOSTER.

* [Global Open Science Hardware Roadmap](http://openhardware.science/global-open-science-hardware-roadmap/), GOSH.

* [Open and Reproducible Science syllabus](https://osf.io/qbm89/) (Campbell, 2018).

* [EQUATOR network](http://www.equator-network.org/) (Enhancing the QUAlity and Transparency Of health Research).

* [Knitr](https://yihui.name/knitr/): Elegant, flexible, and fast dynamic report generation with R (Yihui Xie).

    * [Using Sweave and knitr](https://support.rstudio.com/hc/en-us/articles/200552056-Using-Sweave-and-knitr) (RStudio Support).

* [SoS](http://vatlab.github.io/SoS/), multi-language notebook (based on Jupyter Notebook) and workflow system for cost-effective reproducible analysis.

    * Introduction to [SoS Notebook](https://vatlab.github.io/blog/post/sos-notebook/), [SoS Workflow Engine](https://vatlab.github.io/blog/post/sos-workflow-engine/), and [the power of backing a polyglot notebook with a workflow engine](https://vatlab.github.io/blog/post/power-of-sos-plus-sos-notebook/).
    
* Decentralized research platforms such as [DEIP](https://deip.world/).

### Tasks:

* Find a core data set that is used throughout the examples.

    * If possible, the dataset should have a diverse set of formats and styles for different types of analysis.

* Designing a reproducible research workflow.

    * Create a flowchart of options to help get you started Check if your collaborators, colleagues or supervisors are using the same tools.

    * This can be created as a Google doc and shared for collaboration.

    * Use validated, standardized reagents where possible.

    * Use an electronic lab notebook and best practices for recording protocols and actual steps, reagents used.

* How well annotated are your code scripts? As a general rule of thumb, try and include one comment for every three lines of code. Bear in mind, the primary audience is future you and other people less familiar with your code.

* Posting raw and cleaned data files.

    * Post your data (raw and/or treated) online in a non-proprietary format.

    * Make sure it is in a place where you can get a unique identifier for it.

* Write a study plan or protocol.

    * Preregister your study design using [AsPredicted](https://aspredicted.org/), [OSF](https://osf.io/), or [Registered Reports](https://cos.io/rr/).

    * For clinical trials use [Clinicaltrials.gov](https://clinicaltrials.gov/).

* Set up a reproducible project using an electronic lab notebook to help organise and track your research. 

    * Track changes as your research develops using a version control system such as GitHub.

    * Document everything done by creating a README file.

    * Make sure to select an appropriate license for your repo.

    * Convert the notebook into a standard research manuscript.

    * In this manuscript, include all necessary code to reproduce any figures and tables in their respective captions.